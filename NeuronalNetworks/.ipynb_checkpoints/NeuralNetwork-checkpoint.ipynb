{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "###  forwardPropagation\n",
    "solo debemos multiplicar los pesos $W$ por el input sumar b ,pasar esta suma ponderada por la funcion de activacion e ir avanzando capa por capa\n",
    "### backward Propagation\n",
    "sea la siguiente composicion de funciones:\n",
    "$$C(a(z^{L}))$$\n",
    "donde $C$ es la funcion conste definida como:\n",
    "$$C(a_{j}^{L})=\\frac{1}{2}\\sum_{j}^{}(y_{i}-a_{j}^{L})^{2}$$\n",
    "$a$ la funcion de activacion:\n",
    "$$a^{L}(z^{L})=\\frac{1}{1+e^{-z^{L}}}$$\n",
    "y $z$ la suma ponderada:\n",
    "$$z^{L}=\\sum_{i}^{}a_{i}^{L-1}w_{i}^{L}+b^{L}$$\n",
    "\n",
    "#### Como varia el $coste$ ante un cambio del parametro $W$?\n",
    "el parametro w esta conformado por w y el b\n",
    "$$\\frac{\\partial C}{\\partial w^{L}}=\\frac{\\partial C}{\\partial a^{L}}*\\frac{\\partial a^{L}}{\\partial z^{L}}*\\frac{\\partial z^{L}}{\\partial w^{L}}$$\n",
    "$$\\frac{\\partial C}{\\partial b^{L}}=\\frac{\\partial C}{\\partial a^{L}}*\\frac{\\partial a^{L}}{\\partial z^{L}}*\\frac{\\partial z^{L}}{\\partial b^{L}}$$\n",
    "Ahora resolvemos esas derivadas parciales:\n",
    "derivada del coste con respecto al la funcion de activacion:\n",
    "$$\\frac{\\partial C}{\\partial a^{L}}=(a_{j}^{L}-y_{j})$$\n",
    "derivada de la funcion de activacion con respecto a la suma ponderada:\n",
    "$$\\frac{\\partial a^{L}}{\\partial z^{L}}=a^{L}(z^{L})*(1-a^{L}(z^{L}))$$\n",
    "derivada de la suma ponderada con respecto a w:\n",
    "$$\\frac{\\partial z^{L}}{\\partial b^{L}}=a_{i}^{L-1}$$\n",
    "derivada de la suma ponderada con respecto a b:\n",
    "$$\\frac{\\partial z^{L}}{\\partial w^{L}}=1$$\n",
    "\n",
    "### Algoritmo de Backpropagation\n",
    "1. Computo del error de la ultima capa\n",
    "    $$\\delta^{L}=\\frac{\\partial C}{\\partial a^{L}}*\\frac{\\partial a^{L}}{\\partial z^{L}}$$\n",
    "2. Retropropagamos el error a la capa anterior\n",
    "    $$\\delta^{L-1}=W^{L}*\\delta^{L}*\\frac{\\partial a^{L-1}}{\\partial z^{L-1}}$$\n",
    "3. calculamos las derivadas de la capa usando el error\n",
    "    $$\\frac{\\partial C}{\\partial b^{L-1}}=\\delta^{L-1}$$\n",
    "    $$\\frac{\\partial C}{\\partial w^{L-1}}=\\delta^{L-1}*a^{L-2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "class ReadData(object):\n",
    "    def __init__(self,datasetName='Iris.csv'):\n",
    "        self.datasetName=datasetName\n",
    "    def readData(self):\n",
    "        df = pd.read_csv('Iris.csv')   \n",
    "        df = df.drop(['Id'],axis=1)\n",
    "        #rows = list(range(100,150))\n",
    "        #df = df.drop(df.index[rows]) \n",
    "        Y = []\n",
    "        target = df['Species']\n",
    "        for val in target:\n",
    "            if(val == 'Iris-setosa'):\n",
    "                Y.append(0)\n",
    "            elif(val=='Iris-versicolor'):\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(2)\n",
    "        df = df.drop(['Species'],axis=1)\n",
    "        X = df.values.tolist()\n",
    "        datafeatureSize=50\n",
    "        labels = np.array([0]*datafeatureSize + [1]*datafeatureSize + [2]*datafeatureSize)\n",
    "        Y = np.zeros((datafeatureSize*3, 3))\n",
    "        for i in range(datafeatureSize*3):  \n",
    "            Y[i, labels[i]] = 1\n",
    "        X, Y = shuffle(X,Y)\n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "        forTestY=Y[105:]\n",
    "        forTestX=X[105:,:]\n",
    "        X=X[0:105,:]\n",
    "        Y=Y[0:105,:]\n",
    "        return X,Y,forTestX,forTestY\n",
    "r=ReadData()\n",
    "r.readData()\n",
    "[X,Y,forTestX,forTestY]=r.readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida  i= 0 [[0.00885736 0.26933124 0.78605645]] salida deseada [0. 1. 0.]\n",
      "salida  i= 1 [[0.00871677 0.26743387 0.7779522 ]] salida deseada [0. 1. 0.]\n",
      "salida  i= 2 [[0.98740766 0.00882241 0.00681462]] salida deseada [1. 0. 0.]\n",
      "salida  i= 3 [[0.0089302  0.26931213 0.78789021]] salida deseada [0. 0. 1.]\n",
      "salida  i= 4 [[0.00888303 0.26946637 0.78693553]] salida deseada [0. 0. 1.]\n",
      "salida  i= 5 [[0.98581868 0.00858015 0.00727203]] salida deseada [1. 0. 0.]\n",
      "salida  i= 6 [[0.00894254 0.26928526 0.78817665]] salida deseada [0. 0. 1.]\n",
      "salida  i= 7 [[0.00895027 0.26932271 0.78833805]] salida deseada [0. 0. 1.]\n",
      "salida  i= 8 [[0.00877845 0.26974695 0.78428218]] salida deseada [0. 1. 0.]\n",
      "salida  i= 9 [[0.00891633 0.26949309 0.78744512]] salida deseada [0. 1. 0.]\n",
      "salida  i= 10 [[0.00888543 0.26938072 0.78675778]] salida deseada [0. 1. 0.]\n",
      "salida  i= 11 [[0.97779718 0.00794672 0.01109734]] salida deseada [1. 0. 0.]\n",
      "salida  i= 12 [[0.96854073 0.00736767 0.01749574]] salida deseada [1. 0. 0.]\n",
      "salida  i= 13 [[0.00895664 0.26948074 0.78838249]] salida deseada [0. 0. 1.]\n",
      "salida  i= 14 [[0.00894662 0.26932184 0.78826036]] salida deseada [0. 0. 1.]\n",
      "salida  i= 15 [[0.0088594  0.26947934 0.78619614]] salida deseada [0. 1. 0.]\n",
      "salida  i= 16 [[0.0089041  0.26942832 0.78730434]] salida deseada [0. 1. 0.]\n",
      "salida  i= 17 [[0.98881424 0.00980454 0.005465  ]] salida deseada [1. 0. 0.]\n",
      "salida  i= 18 [[0.98916762 0.00997329 0.00535581]] salida deseada [1. 0. 0.]\n",
      "salida  i= 19 [[0.00887577 0.26970574 0.78645448]] salida deseada [0. 1. 0.]\n",
      "salida  i= 20 [[0.0088223  0.26941245 0.7854058 ]] salida deseada [0. 1. 0.]\n",
      "salida  i= 21 [[0.00880511 0.269534   0.78453033]] salida deseada [0. 1. 0.]\n",
      "salida  i= 22 [[0.00894012 0.26928094 0.78811908]] salida deseada [0. 0. 1.]\n",
      "salida  i= 23 [[0.00885714 0.26942823 0.78618624]] salida deseada [0. 1. 0.]\n",
      "salida  i= 24 [[0.99473721 0.0175937  0.00211385]] salida deseada [1. 0. 0.]\n",
      "salida  i= 25 [[0.00895127 0.26935152 0.78831099]] salida deseada [0. 0. 1.]\n",
      "salida  i= 26 [[0.0088086  0.26992555 0.78476013]] salida deseada [0. 1. 0.]\n",
      "salida  i= 27 [[0.00896027 0.26934837 0.78849403]] salida deseada [0. 0. 1.]\n",
      "salida  i= 28 [[0.95752802 0.00737744 0.02055512]] salida deseada [1. 0. 0.]\n",
      "salida  i= 29 [[0.98625472 0.00865597 0.00702881]] salida deseada [1. 0. 0.]\n",
      "salida  i= 30 [[0.9883604  0.01011107 0.00556415]] salida deseada [1. 0. 0.]\n",
      "salida  i= 31 [[0.98461569 0.00854829 0.00745665]] salida deseada [1. 0. 0.]\n",
      "salida  i= 32 [[0.0089122  0.26946634 0.78740663]] salida deseada [0. 0. 1.]\n",
      "salida  i= 33 [[0.9889921  0.00988627 0.00541001]] salida deseada [1. 0. 0.]\n",
      "salida  i= 34 [[0.99145119 0.01323243 0.00357156]] salida deseada [1. 0. 0.]\n",
      "salida  i= 35 [[0.00877945 0.26923848 0.78381686]] salida deseada [0. 1. 0.]\n",
      "salida  i= 36 [[0.00895607 0.26927024 0.78847188]] salida deseada [0. 0. 1.]\n",
      "salida  i= 37 [[0.0088423  0.26949302 0.7856151 ]] salida deseada [0. 1. 0.]\n",
      "salida  i= 38 [[0.98764674 0.00939302 0.00609105]] salida deseada [1. 0. 0.]\n",
      "salida  i= 39 [[0.0088312  0.26956976 0.78555559]] salida deseada [0. 1. 0.]\n",
      "salida  i= 40 [[0.00891463 0.2694308  0.78748096]] salida deseada [0. 0. 1.]\n",
      "salida  i= 41 [[0.98845717 0.00928173 0.00588423]] salida deseada [1. 0. 0.]\n",
      "salida  i= 42 [[0.98809758 0.0094079  0.00615344]] salida deseada [1. 0. 0.]\n",
      "salida  i= 43 [[0.00893324 0.26933727 0.78794327]] salida deseada [0. 0. 1.]\n",
      "salida  i= 44 [[0.98997061 0.01066805 0.00496975]] salida deseada [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NeuralLayer(object):#clase capa neuronal\n",
    "    def __init__(self,numberConections,numberNeurons,activationFunction):\n",
    "        self.numberConections=numberConections\n",
    "        self.numberNeurons=numberNeurons\n",
    "        self.activationFunction=activationFunction\n",
    "        self.bayas=np.random.rand(1,numberNeurons)*2-1#inicializacion con random\n",
    "        self.W=np.random.rand(numberConections,numberNeurons)*2-1#inicializacion con random    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self,learningRatio=0.01,train=True,numIterations=1000,topology=[4,3,1]):\n",
    "        self.learningRatio=learningRatio\n",
    "        self.train=train\n",
    "        self.numIterations=numIterations\n",
    "        self.topology=topology\n",
    "        self.neuralNetwork=self.createNeuralNetwork()\n",
    "    def createNeuralNetwork(self):\n",
    "        nn=[]\n",
    "        for i,layer in enumerate(self.topology[:-1]):#itera hasta len(topology)-1\n",
    "            nn.append(NeuralLayer(self.topology[i],self.topology[i+1],self.sigmoide))#crea un objeto neuralLayer\n",
    "        return nn\n",
    "    sigmoide=(lambda x:1/(1+np.e**(-x)),lambda x:x*(1-x)) #funcion de activacion mas su rerivada\n",
    "    costFunction=(lambda yp,yr:np.mean((yp-yr)**2),\n",
    "                 lambda yp,yr:(yp-yr))#funcion de costo mas su rerivada\n",
    "    def forwardPropagation(self,X,Y):\n",
    "        out=[(None,X)]#tupla None,X \n",
    "        for i,layer in enumerate(self.neuralNetwork):\n",
    "            z=out[-1][1]@self.neuralNetwork[i].W+self.neuralNetwork[i].bayas\n",
    "            a=self.neuralNetwork[i].activationFunction[0](z)\n",
    "            out.append((z,a))#se agrega una nueva tupla confotmado de (z,a) donde z es la suma ponderada\n",
    "                             #y a es resultado de pasar z como parametro por la funcion de activacion   \n",
    "        return out \n",
    "    def backPropagation(self,X,Y):\n",
    "        out=self.forwardPropagation(X,Y)\n",
    "        if self.train:\n",
    "            deltas=[]\n",
    "            for i in reversed(range(0, len(self.neuralNetwork))):\n",
    "                a=out[i+1][1]\n",
    "                z=out[i+1][0]\n",
    "                if i==len(self.neuralNetwork)-1:#para la ultima capa\n",
    "                    deltas.insert(0,self.costFunction[1](a,Y)*self.neuralNetwork[i].activationFunction[1](a))\n",
    "                else:#para las demas capas\n",
    "                    deltas.insert(0, deltas[0] @ _W.T * self.neuralNetwork[i].activationFunction[1](a))\n",
    "                _W=self.neuralNetwork[i].W\n",
    "                ##desenso del gradiente\n",
    "                self.neuralNetwork[i].bayas=self.neuralNetwork[i].bayas-np.mean(deltas[0],axis=0,keepdims=True)*self.learningRatio\n",
    "                self.neuralNetwork[i].W=self.neuralNetwork[i].W-out[i][1].T@deltas[0]*self.learningRatio\n",
    "        return out[-1][1]\n",
    "    def fit(self,X,Y):\n",
    "        loss=[]\n",
    "        for i in range(self.numIterations):\n",
    "            out=self.backPropagation(X,Y)\n",
    "            loss.append(self.costFunction[0](out,Y))\n",
    "            clear_output(wait=True)\n",
    "        #plt.plot(range(len(loss)), loss)\n",
    "        #plt.show()\n",
    "        return loss\n",
    "    def predict(self,X,Y):\n",
    "        confusionMatrix=[[0,0,0],[0,0,0],[0,0,0]]\n",
    "        outPut=[]\n",
    "        for i in range(X.shape[0]):\n",
    "            out=self.forwardPropagation(X[i:i+1,:],Y[i])\n",
    "            outPut.append(out[-1][1])\n",
    "            #print(\"salida \",\"i=\",i,out[-1][1],\"salida deseada\",Y[i]) \n",
    "            for j in range(len(confusionMatrix)):\n",
    "                if Y[i][j]==0 and outPut[i][j]<0.5:\n",
    "                    confusionMatrix[i][j]=confusionMatrix[i][j]+1\n",
    "                elif Y[i][j]==1 and outPut[i][j]>=0.5  \n",
    "        '''if outPut[i]>0.5 and Y[i]==1:\n",
    "                confusionMatrix[0][0]=confusionMatrix[0][0]+1\n",
    "            elif outPut[i]<=0.5 and Y[i]==1:\n",
    "                confusionMatrix[0][1]=confusionMatrix[0][1]+1\n",
    "            elif outPut[i]<=0.5 and Y[i]==0:\n",
    "                confusionMatrix[1][1]=confusionMatrix[1][1]+1\n",
    "            elif outPut[i]>0.5 and Y[i]==0:\n",
    "                confusionMatrix[1][0]=confusionMatrix[1][0]+1\n",
    "        print(confusionMatrix)\n",
    "        cm_df = pd.DataFrame(confusionMatrix,\n",
    "                     index = ['setosa','versicolor'], \n",
    "                     columns = ['setosa','versicolor'])\n",
    "        #sns.heatmap(cm_df, annot=True)\n",
    "        #plt.show()\n",
    "        N = len(Y)\n",
    "        x = range(N)\n",
    "        xx=np.array(x)\n",
    "        xx=xx+0.35\n",
    "        width = 1/1.5\n",
    "        plt.bar(x,Y,width=0.35, color=\"blue\")\n",
    "        plt.bar(xx,outPut,width=0.35, color=\"red\")     \n",
    "        plt.legend([\"Y\",\"Y predicho\"])'''\n",
    "def test(topology,lr):\n",
    "    losts=[]\n",
    "    for i in range(len(topology)):\n",
    "        nn=NeuralNetwork(learningRatio=lr,topology=topology[i],numIterations=1000)\n",
    "        losts.append(nn.fit(X,Y))\n",
    "    labels=['topologia[4,4,3]','topologia[4,6,3]','topologia[4,8,3]','topologia[4,10,3]','topologia[4,12,3]']\n",
    "    for i in range(len(losts)):\n",
    "        plt.plot(range(len(losts[i])), losts[i], label=labels[i])\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('error')\n",
    "    plt.title(\"learning ratio=0.07\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "     \n",
    "if __name__=='__main__':\n",
    "    #topologys=[[4,4,3],[4,6,3],[4,8,3],[4,10,3],[4,12,3]]\n",
    "    #test(topologys,0.07)\n",
    "    nn1=NeuralNetwork(learningRatio=0.101,topology=[4,12,3],numIterations=1000)\n",
    "    loss=nn1.fit(X,Y)\n",
    "    nn1.predict(forTestX,forTestY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49671415, -3.1382643 ],\n",
       "       [ 0.64768854, -1.47697014],\n",
       "       [-0.23415337, -3.23413696],\n",
       "       ...,\n",
       "       [-0.36532155, -2.81531969],\n",
       "       [-1.34712629, -3.97161404],\n",
       "       [ 1.20041391, -3.65689428]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
